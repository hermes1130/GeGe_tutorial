---
title: "Tutorial_scRNAseqAnalysis"
author: "Jeong-Eun Lee and Melanie Sarfert"
output:
  html_document: 
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### Required packages
Please install the following packages before you start this tutorial: 

* **here**: https://cran.r-project.org/web/packages/here/index.html

* **dplyr**: https://www.r-project.org/nosvn/pandoc/dplyr.html

* **Seurat**: https://satijalab.org/seurat/articles/install.html

#### Tutorial files
Please download a [GitHub repository](https://github.com/hermes1130/GeGe_tutorial) for both of tutorial days


# Short background for the dataset

For this tutorial, we will be analyzing the a dataset of scRNA-seq (10x Genomics) to profile cell composition across a time course of human/chimpanzee organoid development from pluripotency to four months using embryonic stem cells (H9) and iPSC (409b2) lines and chimpanzee iPSC (Sandra/SandraA) lines ([Kanton et al. 2019](https://www.nature.com/articles/s41586-019-1654-9)). This means, the dataset contains multiple readcount metrics per cells including the timepoint 

![Differentiation of human and chimpanzee cerebral organoids.](https://media.springernature.com/full/springer-static/esm/art%3A10.1038%2Fs41586-019-1654-9/MediaObjects/41586_2019_1654_Fig5_ESM.jpg?as=webp)

# Setup the Seurat Object

Load the library here and Seurat package

```{r message=FALSE}
library("here")
library("dplyr")
library("Seurat")
```

We start by reading in the data. The readRDS() function reads Large Seurat files in the output of the Seurat pipeline, returning a S4 SeuratObject. The values in assays represent the multiple read count tables for each feature (i.e. gene; row) that are detected in each cell (column). For example, hu[["RNA"]]\@counts

Let's import the rds files using here 
```{r eval = FALSE}
hu <- readRDS("files/timecourse_human_singlecells_GRCh38.rds")
hu <- subset(x = hu, downsample = 100, invert = TRUE)
#100 cells in each cell type and remove idents with invert
```

```{r cars}
hu <- readRDS("files/Tutorial_tc_human_integ_RNAassay.rds")
hu

ch <- readRDS("files/Tutorial_tc_chimp_integ_RNAassay.rds")
ch
```
Explore the Large Seurat objects

```{r}
head(hu@meta.data, 5)
hu[["RNA"]]@counts[1:10, 1:30]

head(ch@meta.data, 5)
ch[["RNA"]]@counts[1:10, 200:230]
```

As you see, those rds files are already pre-processed, for instance cell lines are already integrated and the linear/non-linear dimensional reduction has already been performed. We'll now split the datasets, so we can practice the whole process to learn how to run a standard Seurat clustering pipeline.

Split the dataset into a list of two seurat objects by cell line.
```{r}
Split_hu <- SplitObject(hu, split.by = "line")
Split_hu
Split_ch <- SplitObject(ch, split.by = "line")
Split_ch
```

After spliting cell lines from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in NorFea_hu$H9[["RNA"]]\@data. 

We next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets. This is implemented in the FindVariableFeatures() function. By default, we return 2,000 features per dataset. These will be used in downstream analysis, like PCA.

The normalization and identification of variable features for each dataset independently will be done in one commmend using laaply(). 

```{r warning = FALSE}
NorFea_hu <- lapply(X = Split_hu, FUN = function(x) {
  x <- NormalizeData(x)
  x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})
NorFea_hu
NorFea_hu$H9[["RNA"]]@data[1:10, 1:30]

VFP_H9 <- VariableFeaturePlot(NorFea_hu$H9)
VFP_H9 <- LabelPoints(plot = VFP_H9, points = head(VariableFeatures(NorFea_hu$H9), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_h209B2 <- VariableFeaturePlot(NorFea_hu$h409B2)
VFP_h209B2 <- LabelPoints(plot = VFP_h209B2, points = head(VariableFeatures(NorFea_hu$h409B2), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_H9
VFP_h209B2

NorFea_ch <- lapply(X = Split_ch, FUN = function(x) {
  x <- NormalizeData(x)
  x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})
NorFea_ch
NorFea_ch$SandraA[["RNA"]]@data[1:10, 200:230]

VFP_SA <- VariableFeaturePlot(NorFea_ch$SandraA)
VFP_SA <- LabelPoints(plot = VFP_SA, points = head(VariableFeatures(NorFea_ch$SandraA), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_S <- VariableFeaturePlot(NorFea_ch$Sandra)
VFP_S <- LabelPoints(plot = VFP_S, points = head(VariableFeatures(NorFea_ch$Sandra), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_SA
VFP_S
```
Now, we see that the number of variable features has been changed to 2000 in each cell line. We will now select features that are repeatedly variable across datasets for integration.

```{r}
features_h <- SelectIntegrationFeatures(object.list = NorFea_hu)
head(features_h, n = 10)

features_c <- SelectIntegrationFeatures(object.list = NorFea_ch)
head(features_c, n = 10)
```

# Perform integration

We then identify anchors using the FindIntegrationAnchors() function, which takes a list of Seurat objects as input, and use these anchors to integrate the two datasets together with IntegrateData(). 

```{r}
anchors_hu <- FindIntegrationAnchors(object.list = NorFea_hu, anchor.features = features_h)
anchors_hu
anchors_ch <- FindIntegrationAnchors(object.list = NorFea_ch, anchor.features = features_c)
anchors_ch
```

We then pass these anchors to the IntegrateData() function, which returns a Seurat object.

The returned object will contain a new Assay, which holds an integrated (or ‘batch-corrected’) expression matrix for all cells, enabling them to be jointly analyzed.

```{r}
hu_integ <- IntegrateData(anchorset = anchors_hu)
hu_integ
ch_integ <- IntegrateData(anchorset = anchors_ch)
ch_integ
```
Now, the number of features has also been changed to 2000 in each object and the active assay is set to 'integrated'. The original unmodified data still resides in the 'RNA' assay. 

# Scaling the data

As this integrated assay is already normalized and the variable features are identified, we can continue with scaling, a linear transformation that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The ScaleData() function shifts the expression of each gene, so that the mean expression across cells is 0 and scales the expression of each gene, so that the variance across cells is 1, so highly-expressed genes do not dominate. The results of this are stored in hu_integ[["RNA"]]\@scale.data


```{r}
hu_integ <- ScaleData(hu_integ, verbose = FALSE)
ch_integ <- ScaleData(ch_integ, verbose = FALSE)
```
# Perform linear dimensional reduction
Next, we perform PCA on the scaled data. By default, only the previously determined variable features are used as input and we define the number of PCs to compute and store (50 by default). 

```{r}
hu_integ <- RunPCA(hu_integ, verbose = FALSE)
hu_integ
ch_integ <- RunPCA(ch_integ, verbose = FALSE)
ch_integ
```
Now, the Seurat object contains one dimensional reduction: pca.

# Determine the dimensionality of the dataset
To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a ‘metafeature’ that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. However, how many components should we choose to include? To answer this question, we'll consider three approaches below.

### First approach
Seurat provides several useful ways of visualizing both cells and features that define the PCA, including VizDimReduction(), DimPlot(), and DimHeatmap(). In particular **DimHeatmap()** allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. 

For instance, let's visualize the first and last PC.

```{r}
DimHeatmap(hu_integ, dims = c(1,50), cells = 500, balanced = TRUE)
DimHeatmap(ch_integ, dims = c(1,50), cells = 500, balanced = TRUE)
```


The first PC shows a clear heterogeneity, while the last one doesn't. The cutoff should be somewhere between PC1 and PC50 (wink wink ;))

### Second approach

As next approach, we randomly permute a subset of the data (1% by default) and rerun PCA, constructing a ‘null distribution’ of feature scores, and repeat this procedure. We identify ‘significant’ PCs as those who have a strong enrichment of low p-value features. This implements a statistical test based on a random null model, but is time-consuming for large datasets. 

Determine stastistical significance of PCA scores using JackStraw(), compute JackStraw scores significance using ScoreJAckStraw. JackStrawPlot() function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). ‘Significant’ PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line).

This takes up to 5 min
```{r}
JS_hu <- JackStraw(hu_integ, dims = 50, num.replicate = 100)
JS_hu <- ScoreJackStraw(JS_hu, dims = 1:50)
JackStrawPlot(JS_hu, dims = c(1, 50))

JS_ch <- JackStraw(ch_integ, dims = 50, num.replicate = 100)
JS_ch <- ScoreJackStraw(JS_ch, dims = 1:50)
JackStrawPlot(JS_ch, dims = c(1, 50))
```


Although both first and last PCs have a significant p-value (< 0.05), the PC1 shows a stronger enrichment of features with low p-values. The cutoff should be somewhere between PC1 and PC50 (wink wink ;))

### Last approach

Last approach is an alternative heuristic method generating an **Elbow plot**, which ranks the principle components based on the percentage of variance explanined by each one. 


```{r}
ElbowPlot(hu_integ, ndims = 50)
ElbowPlot(ch_integ, ndims = 50)
```


## Excercises

Which PC should be the threshold to define the cutoff?

Hint1: Some are difficult to distinguish from background noise for a dataset of this size without prior knowledge. We advise to set the cutoff on the higher side.
Hint2: JackStrawPlot() may not return a clear PC cutoff, please focus on a sharp drop-off in significance.
Hint3: Please take a look in how many PCs the majority of true signal is captured in ElbowPlot()

After deciding the parameter, rerun the RunPCA() with defined npcs.
```{r eval=FALSE}
hu_integ <- RunPCA(hu_integ, npcs = [number of pca of your choice], verbose = FALSE)
ch_integ <- RunPCA(ch_integ, npcs = [number of pca of your choice], verbose = FALSE)
```

```{r include=FALSE}
hu_integ <- RunPCA(hu_integ, npcs = 20, verbose = FALSE)
ch_integ <- RunPCA(ch_integ, npcs = 20, verbose = FALSE)
```

# Cluster the cells
we first construct a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods. This step is performed using the FindNeighbors() function, and takes as input the previously defined dimensionality of the dataset (first x PCs how you decided).

The FindClusters() function iteratively groups cells together, with the goal of optimizing the standard modularity function, and contains a resolution parameter that sets the ‘granularity’ of the downstream clustering, with increased values leading to a greater number of clusters. The clusters can be found using the Idents() function.

```{r eval=FALSE}
hu_integ <- FindNeighbors(hu_integ, dims = 1:[number of pca of your choice])
hu_integ <- FindClusters(hu_integ, resolution = 0.6)

ch_integ <- FindNeighbors(ch_integ, dims = 1:[number of pca of your choice])
ch_integ <- FindClusters(ch_integ, resolution = 0.6)
```

```{r echo=FALSE }
hu_integ <- FindNeighbors(hu_integ, dims = 1:20)
hu_integ <- FindClusters(hu_integ, resolution = 0.6)

ch_integ <- FindNeighbors(ch_integ, dims = 1:20)
ch_integ <- FindClusters(ch_integ, resolution = 0.6)

```
Look at cluster IDs of the first 5 cells and check how many clusters (=levels) are found.

```{r}
head(Idents(hu_integ), 5)
head(Idents(ch_integ), 5)
```

# Run non-linear dimensional reduction (UMAP/tSNE)

Seurat offers two major non-linear dimensional reduction techniques, such as tSNE and UMAP, to visualize and explore these datasets. The goal of these algorithms is to learn the underlying manifold of the data in order to place similar cells together in low-dimensional space. Cells within the graph-based clusters determined above should co-localize on these dimension reduction plots. As input to the UMAP and tSNE, we'll use the same PCs as input to the clustering analysis.

### UMAP (Uniform Manifold Approximation and Projection)

```{r eval=FALSE}
hu_integ <- RunUMAP(hu_integ, reduction = "pca", dims = 1:[number of pca of your choice])
ch_integ <- RunUMAP(ch_integ, reduction = "pca", dims = 1:[number of pca of your choice])
```

```{r echo=FALSE }
hu_integ <- RunUMAP(hu_integ, reduction = "pca", dims = 1:20)
ch_integ <- RunUMAP(ch_integ, reduction = "pca", dims = 1:20)
```

```{r}
DimPlot(hu_integ, reduction = "umap", group.by = "line")
DimPlot(hu_integ, reduction = "umap", split.by = "line")
DimPlot(hu_integ, reduction = "umap")

DimPlot(ch_integ, reduction = "umap", group.by = "line")
DimPlot(ch_integ, reduction = "umap", split.by = "line")
DimPlot(ch_integ, reduction = "umap")
```

### tSNE (t-distributed stochastic neighbor embedding)

```{r eval=FALSE}
hu_integ <- RunTSNE(hu_integ, reduction = "pca", dims = 1:[number of pca of your choice])
ch_integ <- RunTSNE(ch_integ, reduction = "pca", dims = 1:[number of pca of your choice])
```

```{r echo=FALSE }
hu_integ <- RunTSNE(hu_integ, reduction = "pca", dims = 1:20)
ch_integ <- RunTSNE(ch_integ, reduction = "pca", dims = 1:20)
```

```{r}
DimPlot(hu_integ, reduction = "tsne", group.by = "line")
DimPlot(hu_integ, reduction = "tsne", split.by = "line")
DimPlot(hu_integ, reduction = "tsne")

DimPlot(ch_integ, reduction = "tsne", group.by = "line")
DimPlot(ch_integ, reduction = "tsne", split.by = "line")
DimPlot(ch_integ, reduction = "tsne")

```
## Excercises

1. Try to cluster the cells with a different number of PCs (10, 15, or even 50!). Do you observe a dramatic difference?

2. What differences do you observe between UMAP and tSNE plot?

# Identify conserved cell type markers

Markers define clusters via differential expression. By default, it identifies positive and negative markers of a single cluster (specified in ident.1), compared to all other cells. FindAllMarkers() automates this process for all clusters, but you can also test groups of clusters vs. each other, or against all cells. The FindAllMarkers() function has three important arguments which provide thresholds for determining whether a gene is a marker:

* **min.pct**: This determines the fraction of cells in either group the feature has to be expressed in to be included in the analysis. It's meant to remove lowly expressed genes. The default is 0.1 and this means at least 10 % of cells in cluster x or all other clusters must express the gene. The lower the value, the longer computing time. If this is set to high value, many false positive could be included due to the fact that not all genes are detected in all cells (even if it is expressed)

* **min.diff.pct**: Minimum percent difference between the percent of cells expressing the gene in the cluster and the percent of cells expressing gene in all other clusters combined. This will downsample each identity class to have no more cells than whatever this is set to. The default is -Inf and this means no difference threshold is set. The lower the value, the longer computing time.  

* **logfc.threshold**: Minimum log2 foldchange for average expression of gene in cluster relative to the average expression in all other clusters combined. The default is 0.25 and this means the average log2 foldchange should be at least 0.25. The lower the value, the longer computing time. If this is set to high value, many weak signals could be missed.

For performing differential expression after integration, we switch back to the original
```{r}
DefaultAssay(hu_integ) <- "RNA"
hu_integ

DefaultAssay(ch_integ) <- "RNA"
ch_integ
```
Now, find markers for every cluster compared to all remaining cells, report only the positive ones. 

```{r}
Markers_hu <- FindAllMarkers(hu_integ, only.pos = TRUE, min.pct = 0.25,  logfc.threshold = 0.25)
Markers_hu
Markers_ch <- FindAllMarkers(ch_integ, only.pos = TRUE, min.pct = 0.25,  logfc.threshold = 0.25)
Markers_ch
```
Check if you have the average log2 foldchange lower than 0.25 and how many markers are detected in each cluster.

```{r}
sum(Markers_hu$avg_log2FC < 0.25)
Markers_hu %>% count(cluster)
sum(Markers_ch$avg_log2FC < 0.25)
Markers_ch %>% count(cluster)
```

Let's take a look into the marker expression. For that, we'll collect top markers with lowest p-value per cluster

```{r}
top2_hu <- Markers_hu %>%
  group_by(cluster) %>%
  top_n(n = 2, wt = avg_log2FC)
top2_hu

top2_ch <- Markers_ch %>%
  group_by(cluster) %>%
  top_n(n = 2, wt = avg_log2FC)
top2_ch
```

Seurat package provides several tools for visualizing marker expression. VlnPlot() (shows expression probability distributions across clusters), and FeaturePlot() (visualizes feature expression on a tSNE or PCA plot) are our most commonly used visualizations. Also, RidgePlot(), CellScatter(), and DotPlot() are available.

Here, we look at the expression level of the top 2 markers with lowest p-value in cluster 0. In case of using FeaturePlot(), the default is set to umap reduction. 

```{r}
VlnPlot(hu_integ, features = top2_hu %>% filter(cluster %in% "0") %>% pull(-1))
VlnPlot(ch_integ, features = top2_ch %>% filter(cluster %in% "0") %>% pull(-1))
RidgePlot(hu_integ, features = top2_hu %>% filter(cluster %in% "0") %>% pull(-1))
RidgePlot(ch_integ, features = top2_ch %>% filter(cluster %in% "0") %>% pull(-1))

FeaturePlot(hu_integ, features = top2_hu %>% filter(cluster %in% "0") %>% pull(-1))
DimPlot(hu_integ, reduction = "umap")
FeaturePlot(ch_integ, features = top2_ch %>% filter(cluster %in% "0") %>% pull(-1))
DimPlot(ch_integ, reduction = "umap")

FeaturePlot(hu_integ, features = top2_hu %>% filter(cluster %in% "0") %>% pull(-1), reduction = "tsne")
DimPlot(hu_integ, reduction = "tsne")
FeaturePlot(ch_integ, features = top2_ch %>% filter(cluster %in% "0") %>% pull(-1), reduction = "tsne")
DimPlot(ch_integ, reduction = "tsne")
```
Using Dotplot(), we can get a more comprehensive overview of marker expressions by providing the top 2 markers in all clusters. 

```{r}
DotPlot(hu_integ, features = unique(top2_hu %>%  pull(-1)), cols = c("blue", "red"), dot.scale = 8) +
    RotatedAxis()
DotPlot(ch_integ, features = unique(top2_ch %>%  pull(-1)), cols = c("blue", "red"), dot.scale = 8) +
    RotatedAxis()
```

## Excercises

Play with the three major arguments of FindAllMarkers() function! For each task, check the computing time with Sys.time() and visualize the marker expression with a different number of top markers (2, 5, or even 10!)

1. Try to include more genes per cluster by adjusting the minimun value of the average log2 foldchange. Does it take longer than the code in the tutorial? How many markers are detected in each cluster? Do clusters contain more specific markers? 

2. Try to include less genes per cluster by excluding lowly expressed genes with less than 50 % of cells in cluster x or all other clusters must express the gene. Does it take longer than the code in the tutorial? How many markers are detected in each cluster? Do clusters contain more specific markers? 

3. Try to include more cluster-specifically expressed markers by adjusting the minimum difference between two groups. Does it take longer than the code in the tutorial? How many markers are detected in each cluster? Do clusters contain more specific markers? 

# Assigning cell type identity to clusters

After identifying cell type markers in each cluster, it's also important to assign cell type identity to clusters. Getting canonical markers to known cell types still challenging. Depending on source of cells, e.g. brain or heart, or species, e.g. human or drosophila, there're various databases and studies for markers. In this tutorial, we'll use a table containing cell types and corresponding markers based on a previous scRNA-seq study.

Import the table containig marker information
```{r}
CTmarkers <- read.table("files/markers.txt", sep = "\t", header = T)
dim(CTmarkers)
head(CTmarkers)
```
We'll collect top 10 markers with lowest p-value and this will be compared with the imported table
```{r}
top10_hu <- Markers_hu %>%
  group_by(cluster) %>%
  top_n(n = 10, wt = avg_log2FC)
top10_hu
top10_ch <- Markers_ch %>%
  group_by(cluster) %>%
  top_n(n = 10, wt = avg_log2FC)
top10_ch
```
We'll now create a custom function to search the most matching cell type to our markers. For each cluster, the function will check how often the top 10 markers match to certain cell types based on the imported table. The mostly matched cell type for each cluster will be returned. 


```{r}
CTretrieve <- function(x){
  cluster <- c()
  for(i in 1:length(levels(x$cluster))){
    top10 <- x %>% filter(cluster %in% levels(x$cluster)[i]) %>% pull(-1)
    cluster <- c(cluster, CTmarkers %>% 
                  filter(gene %in% top10) %>% 
                  count(cluster) %>% 
                  arrange(desc(n)) %>% 
                  slice(1) %>% 
                  pull(1))
  }
  return(cluster)
}

CTretrieve(top10_hu)
CTretrieve(top10_ch)
```
With this, we can rename our clusters. So far, they were 0, 1, 2, ...

```{r}
new.cluster.ids_hu <- CTretrieve(top10_hu)
names(new.cluster.ids_hu) <- levels(hu_integ)
hu_integ <- RenameIdents(hu_integ, new.cluster.ids_hu)
DimPlot(hu_integ, reduction = "umap")
DimPlot(hu_integ, reduction = "tsne")

new.cluster.ids_ch <- CTretrieve(top10_ch)
names(new.cluster.ids_ch) <- levels(ch_integ)
ch_integ <- RenameIdents(ch_integ, new.cluster.ids_ch)
DimPlot(ch_integ, reduction = "umap")
DimPlot(ch_integ, reduction = "tsne")
```


Enough or should we add DE analysis?





