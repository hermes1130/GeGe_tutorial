---
title: "Tutorial_scRNAseqAnalysis"
author: "Jeong-Eun Lee and Melanie Sarfert"
output:
  html_document: 
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### Required packages
Please install the following packages before you start this tutorial: here, Seurat

* **here**: https://cran.r-project.org/web/packages/here/index.html

* **Seurat**: https://satijalab.org/seurat/articles/install.html

#### Tutorial files
Please download a [GitHub repository](https://github.com/hermes1130/GeGe_tutorial) for both of tutorial days


# Short background for the dataset

For this tutorial, we will be analyzing the a dataset of scRNA-seq (10x Genomics) to profile cell composition across a time course of human/chimpanzee organoid development from pluripotency to four months using embryonic stem cells (H9) and iPSC (409b2) lines and chimpanzee iPSC (Sandra/SandraA) lines ([Kanton et al. 2019](https://www.nature.com/articles/s41586-019-1654-9)). This means, the dataset contains multiple readcount metrics per cells including the timepoint 

![Differentiation of human and chimpanzee cerebral organoids.](https://media.springernature.com/full/springer-static/esm/art%3A10.1038%2Fs41586-019-1654-9/MediaObjects/41586_2019_1654_Fig5_ESM.jpg?as=webp)

# Setup the Seurat Object

Load the library here and Seurat package

```{r message=FALSE}
library("here")
library("Seurat")
```

We start by reading in the data. The readRDS() function reads Large Seurat files in the output of the Seurat pipeline, returning a S4 SeuratObject. The values in assays represent the multiple read count tables for each feature (i.e. gene; row) that are detected in each cell (column). For example, hu[["RNA"]]\@counts

Let's import the rds files using here 
```{r eval = FALSE}
hu <- readRDS("files/timecourse_human_singlecells_GRCh38.rds")
hu <- subset(x = hu, downsample = 100, invert = TRUE)
#100 cells in each cell type and remove idents with invert
```

```{r cars}
hu <- readRDS("files/Tutorial_tc_human_integ_RNAassay.rds")
hu

ch <- readRDS("files/Tutorial_tc_chimp_integ_RNAassay.rds")
ch
```
Explore the Large Seurat objects

```{r}
head(hu@meta.data, 5)
hu[["RNA"]]@counts[1:10, 1:30]

head(ch@meta.data, 5)
ch[["RNA"]]@counts[1:10, 200:230]
```

As you see, those rds files are already pre-processed, for instance cell lines are already integrated and the linear/non-linear dimensional reduction has already been performed. We'll now split the datasets, so we can practice the whole process to learn how to run a standard Seurat clustering pipeline.

Split the dataset into a list of two seurat objects by cell line.
```{r}
Split_hu <- SplitObject(hu, split.by = "line")
Split_hu
Split_ch <- SplitObject(ch, split.by = "line")
Split_ch
```

After spliting cell lines from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method “LogNormalize” that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in NorFea_hu$H9[["RNA"]]\@data. 

We next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). Focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets. This is implemented in the FindVariableFeatures() function. By default, we return 2,000 features per dataset. These will be used in downstream analysis, like PCA.

The normalization and identification of variable features for each dataset independently will be done in one commmend using laaply(). 

```{r warning = FALSE}
NorFea_hu <- lapply(X = Split_hu, FUN = function(x) {
  x <- NormalizeData(x)
  x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})
NorFea_hu
NorFea_hu$H9[["RNA"]]@data[1:10, 1:30]

VFP_H9 <- VariableFeaturePlot(NorFea_hu$H9)
VFP_H9 <- LabelPoints(plot = VFP_H9, points = head(VariableFeatures(NorFea_hu$H9), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_h209B2 <- VariableFeaturePlot(NorFea_hu$h409B2)
VFP_h209B2 <- LabelPoints(plot = VFP_h209B2, points = head(VariableFeatures(NorFea_hu$h409B2), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_H9
VFP_h209B2

NorFea_ch <- lapply(X = Split_ch, FUN = function(x) {
  x <- NormalizeData(x)
  x <- FindVariableFeatures(x, selection.method = "vst", nfeatures = 2000)
})
NorFea_ch
NorFea_ch$SandraA[["RNA"]]@data[1:10, 200:230]

VFP_SA <- VariableFeaturePlot(NorFea_ch$SandraA)
VFP_SA <- LabelPoints(plot = VFP_SA, points = head(VariableFeatures(NorFea_ch$SandraA), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_S <- VariableFeaturePlot(NorFea_ch$Sandra)
VFP_S <- LabelPoints(plot = VFP_S, points = head(VariableFeatures(NorFea_ch$Sandra), 10), repel = TRUE, xnudge = 0, ynudge = 0)
VFP_SA
VFP_S
```
Now, we see that the number of variable features has been changed to 2000 in each cell line. We will now select features that are repeatedly variable across datasets for integration.

```{r}
features_h <- SelectIntegrationFeatures(object.list = NorFea_hu)
head(features_h, n = 10)

features_c <- SelectIntegrationFeatures(object.list = NorFea_ch)
head(features_c, n = 10)
```

# Perform integration

We then identify anchors using the FindIntegrationAnchors() function, which takes a list of Seurat objects as input, and use these anchors to integrate the two datasets together with IntegrateData(). 

```{r}
anchors_hu <- FindIntegrationAnchors(object.list = NorFea_hu, anchor.features = features_h)
anchors_hu
anchors_ch <- FindIntegrationAnchors(object.list = NorFea_ch, anchor.features = features_c)
anchors_ch
```

We then pass these anchors to the IntegrateData() function, which returns a Seurat object.

The returned object will contain a new Assay, which holds an integrated (or ‘batch-corrected’) expression matrix for all cells, enabling them to be jointly analyzed.

```{r}
hu_integ <- IntegrateData(anchorset = anchors_hu)
hu_integ
ch_integ <- IntegrateData(anchorset = anchors_ch)
ch_integ
```
Now, the number of features has also been changed to 2000 in each object and the active assay is set to 'integrated'. The original unmodified data still resides in the 'RNA' assay. 

# Scaling the data

As this integrated assay is already normalized and the variable features are identified, we can continue with scaling, a linear transformation that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The ScaleData() function shifts the expression of each gene, so that the mean expression across cells is 0 and scales the expression of each gene, so that the variance across cells is 1, so highly-expressed genes do not dominate. The results of this are stored in hu_integ[["RNA"]]\@scale.data


```{r}
hu_integ <- ScaleData(hu_integ, verbose = FALSE)
ch_integ <- ScaleData(ch_integ, verbose = FALSE)
```
# Perform linear dimensional reduction
Next, we perform PCA on the scaled data. By default, only the previously determined variable features are used as input and we define the number of PCs to compute and store (50 by default). 

```{r}
hu_integ <- RunPCA(hu_integ, verbose = FALSE)
hu_integ
ch_integ <- RunPCA(ch_integ, verbose = FALSE)
ch_integ
```
Now, the Seurat object contains one dimensional reduction: pca.

# Determine the dimensionality of the dataset
To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a ‘metafeature’ that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. However, how many components should we choose to include? To answer this question, we'll consider three approaches below.

### First approach
Seurat provides several useful ways of visualizing both cells and features that define the PCA, including VizDimReduction(), DimPlot(), and DimHeatmap(). In particular **DimHeatmap()** allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. 

For instance, let's visualize the first and last PC.

```{r}
DimHeatmap(hu_integ, dims = c(1,50), cells = 500, balanced = TRUE)
DimHeatmap(ch_integ, dims = c(1,50), cells = 500, balanced = TRUE)
```


The first PC shows a clear heterogeneity, while the last one doesn't. The cutoff should be somewhere between PC1 and PC50 (wink wink ;))

### Second approach

As next approach, we randomly permute a subset of the data (1% by default) and rerun PCA, constructing a ‘null distribution’ of feature scores, and repeat this procedure. We identify ‘significant’ PCs as those who have a strong enrichment of low p-value features. This implements a statistical test based on a random null model, but is time-consuming for large datasets. 

Determine stastistical significance of PCA scores using JackStraw(), compute JackStraw scores significance using ScoreJAckStraw. JackStrawPlot() function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). ‘Significant’ PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line).

This takes up to 5 min
```{r eval=FALSE}
JS_hu <- JackStraw(hu_integ, dims = 50, num.replicate = 100)
JS_hu <- ScoreJackStraw(JS_hu, dims = 1:50)
JackStrawPlot(JS_hu, dims = c(1, 50))

JS_ch <- JackStraw(ch_integ, dims = 50, num.replicate = 100)
JS_ch <- ScoreJackStraw(JS_ch, dims = 1:50)
JackStrawPlot(JS_ch, dims = c(1, 50))
```


Although both first and last PCs have a significant p-value (< 0.05), the PC1 shows a stronger enrichment of features with low p-values. The cutoff should be somewhere between PC1 and PC50 (wink wink ;))

### Last approach

Last approach is an alternative heuristic method generating an **Elbow plot**, which ranks the principle components based on the percentage of variance explanined by each one. 


```{r}
ElbowPlot(hu_integ, ndims = 50)
ElbowPlot(ch_integ, ndims = 50)
```


## Excercises

Where would you put the cutoff?

Hint1: Some are difficult to distinguish from background noise for a dataset of this size without prior knowledge. We advise to set the cutoff on the higher side.
Hint2: JackStrawPlot() may not return a clear PC cutoff, please focus on a sharp drop-off in significance.
Hint3: Please take a look in how many PCs the majority of true signal is captured in ElbowPlot()

After deciding the parameter, rerun the RunPCA() with defined npcs.
```{r eval=FALSE}
hu_integ <- RunPCA(hu_integ, npcs = [number of pca of your choice], verbose = FALSE)
ch_integ <- RunPCA(ch_integ, npcs = [number of pca of your choice], verbose = FALSE)
```

```{r include=FALSE}
hu_integ <- RunPCA(hu_integ, npcs = 20, verbose = FALSE)
ch_integ <- RunPCA(ch_integ, npcs = 20, verbose = FALSE)
```

# Cluster the cells
we first construct a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods. This step is performed using the FindNeighbors() function, and takes as input the previously defined dimensionality of the dataset (first x PCs how you decided).

The FindClusters() function iteratively groups cells together, with the goal of optimizing the standard modularity function, and contains a resolution parameter that sets the ‘granularity’ of the downstream clustering, with increased values leading to a greater number of clusters. The clusters can be found using the Idents() function.

```{r eval=FALSE}
hu_integ <- FindNeighbors(hu_integ, dims = 1:[number of pca of your choice])
hu_integ <- FindClusters(hu_integ, resolution = 0.6)

ch_integ <- FindNeighbors(ch_integ, dims = 1:[number of pca of your choice])
ch_integ <- FindClusters(ch_integ, resolution = 0.6)
```

```{r echo=FALSE }
hu_integ <- FindNeighbors(hu_integ, dims = 1:20)
hu_integ <- FindClusters(hu_integ, resolution = 0.6)

ch_integ <- FindNeighbors(ch_integ, dims = 1:20)
ch_integ <- FindClusters(ch_integ, resolution = 0.6)

```


```{r eval = FALSE}
tc_human_integ <- ScaleData(tc_human_integ, verbose = FALSE)
tc_human_integ <- RunPCA(tc_human_integ, npcs = 20, verbose = FALSE)
tc_human_integ <- RunTSNE(tc_human_integ, reduction = "pca", dims = 1:20)
tc_human_integ <- FindNeighbors(tc_human_integ, reduction = "pca", dims = 1:20)
tc_human_integ <- FindClusters(tc_human_integ, resolution = 0.6)
```

```{r eval = FALSE}
DimPlot(tc_human_integ, reduction = "tsne", group.by = "line")
DimPlot(tc_human_integ, reduction = "tsne", label = TRUE, repel = TRUE)
```

## Excercises

1. Which method calls more significant genes, DESeq2 or edgeR?

2. How many genes were called differentially expressed with both methods?
Hint1: check out %in%
Hint2: are p-values adjusted for both methods?

3. A venn diagram is also a good way to visualise overlap. Check out the eulerr package (Note, this package is not installed on the evop server, so work with your neighbor who has a PC or Mac).
